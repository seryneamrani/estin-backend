import torch
import json
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel, PeftConfig
import os
import importlib.util

# ğŸ“ Chemin vers le modÃ¨le fine-tunÃ©
BASE_DIR = os.path.dirname(__file__)
MODEL_DIR = os.path.join(BASE_DIR, "trained_model", "trained_model")

# ğŸ“¥ Charger detect_topic depuis injection_utils.py
injection_utils_path = os.path.join(MODEL_DIR, "injection_utils.py")
spec = importlib.util.spec_from_file_location("injection_utils", injection_utils_path)
injection_utils = importlib.util.module_from_spec(spec)
spec.loader.exec_module(injection_utils)
detect_topic = injection_utils.detect_topic

# ğŸ“– Charger les faits
with open(os.path.join(MODEL_DIR, "context_facts.json"), encoding="utf-8") as f:
    context_facts = json.load(f)

# ğŸ”§ Charger config LoRA
config = PeftConfig.from_pretrained(MODEL_DIR)
base_model = AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path)

# ğŸ§  Appliquer le modÃ¨le fine-tunÃ© LoRA
model = PeftModel.from_pretrained(base_model, MODEL_DIR)
tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path, use_fast=True)

# ğŸ“Œ Utiliser CPU si pas de CUDA
device = "cuda" if torch.cuda.is_available() else "cpu"
model = model.to(device)

# ğŸ”® Fonction de test
def generate_estin_response(prompt):
    topics = detect_topic(prompt)
    facts = "\n".join([context_facts[t] for t in topics])
    
    full_prompt = f"""### Instruction:
Tu es un assistant expert de l'ESTIN.
RÃ©ponds uniquement Ã  partir des faits suivants :
{facts}

Question : {prompt.strip()}
ğŸ’¬ RÃ©ponse :"""

    inputs = tokenizer(full_prompt, return_tensors="pt").to(device)
    outputs = model.generate(**inputs, max_new_tokens=300, temperature=0.7)
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return response.split("ğŸ’¬ RÃ©ponse :")[-1].strip()

# ğŸ§ª Test rapide
if __name__ == "__main__":
    question = "OÃ¹ se situe la rÃ©sidence ?"
    reponse = generate_estin_response(question)
    print("RÃ©ponse gÃ©nÃ©rÃ©e :\n", reponse)
